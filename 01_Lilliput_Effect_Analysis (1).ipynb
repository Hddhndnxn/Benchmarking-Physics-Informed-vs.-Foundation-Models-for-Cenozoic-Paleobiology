{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc871a59",
   "metadata": {},
   "source": [
    "# ðŸ”¬ The Lilliput Effect: Climate vs. Taphonomy\n",
    "**Project:** Deep-Time Dynamics\n",
    "**Objective:** To determine if the observed shrinking of marine invertebrates during hyperthermal events (the \"Lilliput Effect\") is a biological response to temperature or a geological artifact of preservation bias.\n",
    "\n",
    "### ðŸ“Š Data Sources\n",
    "* **Fossils:** Paleobiology Database (PBDB) - Occurrences of marine invertebrates.\n",
    "* **Climate:** CENOGRID (Westerhold et al., 2020) - Global Benthic $\\delta^{18}O$ stack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "\n",
    "# 1. Load Fossil Context (Time & Rocks)\n",
    "# Note: 'pbdb_data.csv' headers usually start around line 19\n",
    "try:\n",
    "    df_context = pd.read_csv('pbdb_data.csv', skiprows=19)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'pbdb_data.csv' not found. Please upload the file.\")\n",
    "    # Exit or handle error appropriately in a real notebook\n",
    "    # For now, create an empty dataframe to avoid further errors\n",
    "    df_context = pd.DataFrame()\n",
    "\n",
    "# 2. Load Fossil Measurement File (Sizes)\n",
    "# Note: 'pbdb_data (1).csv' headers usually start around line 17\n",
    "try:\n",
    "    df_meas = pd.read_csv('pbdb_data (1).csv', skiprows=17)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'pbdb_data (1).csv' not found. Please upload the file.\")\n",
    "    df_meas = pd.DataFrame()\n",
    "\n",
    "# 3. Merge them together (The Magic Step)\n",
    "# This matches the ID number from both files\n",
    "if not df_context.empty and not df_meas.empty:\n",
    "    df_final = pd.merge(df_context, df_meas, on='specimen_no', how='inner')\n",
    "    print(\"Success! We have merged Time, Lithology, and Size.\")\n",
    "    print(df_final[['genus', 'max_ma', 'lithology1', 'measurement_type', 'average']].head())\n",
    "else:\n",
    "    df_final = pd.DataFrame()\n",
    "    print(\"Skipping merge due to missing input files.\")\n",
    "\n",
    "# --- 2. Load Your Uploaded Climate Data ---\n",
    "print(\"\\nLoading Climate Data (TableS33.tab)...\")\n",
    "\n",
    "# The file uses tabs (\\t) and has 91 lines of metadata before the table starts.\n",
    "# We skip the metadata and use the standard CENOGRID columns.\n",
    "try:\n",
    "    df_climate = pd.read_csv(\n",
    "        'TableS33.tab',\n",
    "        sep='\\t',\n",
    "        skiprows=91  # Skips the text header to get to the real data\n",
    "    )\n",
    "\n",
    "    # Rename columns to match our project standard\n",
    "    # 'Tuned time [Ma]' -> age_ma\n",
    "    # 'Foram benth Î´18O [â€° PDB] (VPDB CorrAdjusted)' -> d18o (Temperature proxy)\n",
    "    # 'Foram benth Î´13C [â€° PDB] (VPDB CorrAdjusted)' -> d13c (Carbon proxy)\n",
    "    df_climate = df_climate.rename(columns={\n",
    "        'Tuned time [Ma]': 'age_ma',\n",
    "        'Foram benth Î´18O [â€° PDB] (VPDB CorrAdjusted)': 'd18o',\n",
    "        'Foram benth Î´13C [â€° PDB] (VPDB CorrAdjusted)': 'd13c'\n",
    "    })\n",
    "\n",
    "    # Filter for Cenozoic only (0-66 Ma)\n",
    "    df_climate = df_climate[(df_climate['age_ma'] >= 0) & (df_climate['age_ma'] <= 66)]\n",
    "\n",
    "    print(f\"   -> Success! Loaded {len(df_climate)} climate data points.\")\n",
    "    print(f\"   -> Time Range: {df_climate['age_ma'].min():.2f} to {df_climate['age_ma'].max():.2f} Ma\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'TableS33.tab' not found. Please upload the file.\")\n",
    "    df_climate = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error loading climate file: {e}\")\n",
    "\n",
    "# --- 3. The Binning Process (Aligning Fossils to Climate) ---\n",
    "# Ensure you have 'df_final' (your fossil data) from the previous step.\n",
    "if 'df_final' in locals() and not df_final.empty and 'df_climate' in locals() and not df_climate.empty:\n",
    "    print(\"\\nAligning Fossils with Climate...\")\n",
    "\n",
    "    # A. Calculate Fossil Midpoint Age\n",
    "    if 'min_ma' in df_final.columns:\n",
    "        df_final['age_mid'] = (df_final['max_ma'] + df_final['min_ma']) / 2\n",
    "    else:\n",
    "        df_final['age_mid'] = df_final['max_ma']\n",
    "\n",
    "    # B. Define Time Bins (0.1 Ma slices)\n",
    "    # This creates buckets like 45.0, 45.1, 45.2 Ma\n",
    "    bins = np.arange(0, 66.1, 0.1)\n",
    "    labels = bins[:-1] + 0.05\n",
    "\n",
    "    # C. Assign Fossils to Time Bins\n",
    "    df_final['time_bin'] = pd.cut(df_final['age_mid'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # D. Bin climate data and merge d18o into df_final\n",
    "    df_climate['time_bin'] = pd.cut(df_climate['age_ma'], bins=bins, labels=labels, right=False)\n",
    "    # Taking the mean d18o for each bin\n",
    "    df_climate_binned = df_climate.groupby('time_bin', observed=False)['d18o'].mean().reset_index()\n",
    "\n",
    "    # Merge climate data (d18o) into the fossil dataframe\n",
    "    # Using a left merge to keep all fossils, and add d18o where time_bins match\n",
    "    df_final = pd.merge(df_final, df_climate_binned, on='time_bin', how='left')\n",
    "\n",
    "    print(f\"   -> Successfully merged climate data (d18o) into df_final.\")\n",
    "else:\n",
    "    print(\"Skipping alignment of fossils with climate due to missing data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee9ecb",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Feature Engineering: Preparing Data for AI\n",
    "This section transforms the raw merged data into a structured format suitable for machine learning algorithms. We extract relevant features such as absolute latitude and one-hot encode lithology types to represent rock compositions numerically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PHASE 2: FEATURE ENGINEERING ---\n",
    "# This converts your raw data into a matrix of numbers for the AI.\n",
    "\n",
    "# 1. Create a fresh copy for Machine Learning\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    df_ml = df_final.copy()\n",
    "\n",
    "    # 2. Filter for Realism (Remove bad data)\n",
    "    # We need rows that have both a Body Size (average) and a Temperature (d18o)\n",
    "    df_ml = df_ml.dropna(subset=['average', 'd18o'])\n",
    "    df_ml = df_ml[df_ml['average'] > 0] # Size must be positive\n",
    "\n",
    "    # 3. Geography: Calculate Absolute Latitude\n",
    "    # We care about \"Distance from Equator\" (0 to 90), not North/South (-90 to +90).\n",
    "    # PBDB usually stores this in 'lat' or 'paleolat'.\n",
    "    if 'paleolat' in df_ml.columns:\n",
    "        df_ml['latitude_abs'] = df_ml['paleolat'].abs()\n",
    "    elif 'lat' in df_ml.columns:\n",
    "        df_ml['latitude_abs'] = df_ml['lat'].abs()\n",
    "    else:\n",
    "        print(\"Warning: Latitude column missing. Using 0.\")\n",
    "        df_ml['latitude_abs'] = 0\n",
    "\n",
    "    # 4. Taphonomy: One-Hot Encoding (The \"Rock Type\" Translation)\n",
    "    # This converts \"Sandstone\" into [1, 0, 0] and \"Shale\" into [0, 1, 0]\n",
    "\n",
    "    # A. Clean the text (lowercase, remove punctuation)\n",
    "    df_ml['lith_clean'] = df_ml['lithology1'].astype(str).str.lower().str.replace('\"', '').str.strip()\n",
    "\n",
    "    # B. Simplify: Only keep the Top 10 most common rocks (Group others as \"other\")\n",
    "    top_10_rocks = df_ml['lith_clean'].value_counts().nlargest(10).index\n",
    "    df_ml['lith_simple'] = df_ml['lith_clean'].apply(lambda x: x if x in top_10_rocks else 'other')\n",
    "\n",
    "    # C. Convert to Numbers (Get Dummies)\n",
    "    lith_dummies = pd.get_dummies(df_ml['lith_simple'], prefix='lith')\n",
    "    df_ml = pd.concat([df_ml, lith_dummies], axis=1)\n",
    "\n",
    "    # --- 5. Final Check ---\n",
    "    print(\"Feature Engineering Complete!\")\n",
    "    print(f\"Final Dataset Size: {len(df_ml)} rows\")\n",
    "    print(\"Columns ready for AI:\", ['d18o', 'latitude_abs'] + list(lith_dummies.columns))\n",
    "\n",
    "    # Show the 'Translated' Data\n",
    "    print(df_ml[['genus', 'average', 'd18o', 'lith_sandstone', 'lith_shale']].head(5))\n",
    "else:\n",
    "    df_ml = pd.DataFrame()\n",
    "    print(\"Skipping Feature Engineering due to missing or empty df_final.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758cfed",
   "metadata": {},
   "source": [
    "## ðŸ¤– Machine Learning: Random Forest Analysis\n",
    "Here, we employ a Random Forest Regressor to predict fossil body size based on the engineered features. The primary goal is to identify which factors (climate, geography, or lithology) have the most significant influence on body size variation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cefb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "if 'df_ml' in locals() and not df_ml.empty:\n",
    "    # --- 1. Define Predictors (X) and Target (y) ---\n",
    "    # Target: The average body size of the animal\n",
    "    y = df_ml['average']\n",
    "\n",
    "    # Predictors: Temperature (d18o), Geography (latitude), and Rock Type (lith_*)\n",
    "    # Explicitly select only the numerical and one-hot encoded features\n",
    "    predictor_columns = [\n",
    "        'd18o',\n",
    "        'latitude_abs',\n",
    "    ]\n",
    "\n",
    "    # Dynamically add lithology columns if they exist\n",
    "    lithology_cols = [col for col in df_ml.columns if col.startswith('lith_')]\n",
    "    predictor_columns.extend(lithology_cols)\n",
    "\n",
    "    X = df_ml[predictor_columns]\n",
    "\n",
    "    # (Optional) Verify we only have numbers\n",
    "    print(\"Predictor Columns:\", X.columns.tolist())\n",
    "\n",
    "    # --- 2. Split Data (Train vs. Test) ---\n",
    "    # We keep 20% of the data hidden to test the model's accuracy later\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # --- 3. Train the Random Forest ---\n",
    "    print(\"Training the model... (This might take a moment)\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Evaluate Performance ---\n",
    "    # How well can we predict body size? (1.0 is perfect, 0.0 is random guessing)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f\"Model Accuracy (R^2 Score): {score:.4f}\")\n",
    "\n",
    "    # --- 5. THE RESULTS (Feature Importance) ---\n",
    "    # This is the answer to your Research Question.\n",
    "    importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(\"\\n--- RESEARCH RESULTS: What drives Body Size? ---\")\n",
    "    print(importances)\n",
    "\n",
    "    # --- 6. Interpretation Helper ---\n",
    "    top_feature = importances.iloc[0]['Feature']\n",
    "    print(f\"\\nCONCLUSION: The most dominant factor driving body size is '{top_feature}'.\")\n",
    "\n",
    "    if 'd18o' in top_feature:\n",
    "        print(\"-> RESULT: ECOLOGICAL SIGNAL DETECTED. Climate (Temperature) is the main driver.\")\n",
    "        print(\"   (This supports the 'Lilliput Effect' hypothesis).\")\n",
    "    elif 'latitude_abs' in top_feature:\n",
    "        print(\"-> RESULT: GEOGRAPHIC SIGNAL DETECTED. Latitude is the main driver.\")\n",
    "        print(\"   (This could relate to latitudinal diversity gradients or climate zones).\")\n",
    "    else:\n",
    "        print(\"-> RESULT: TAPHONOMIC/OTHER SIGNAL DETECTED. A lithology or other factor is the main driver.\")\n",
    "        print(\"   (This might indicate preservation bias or other environmental controls).\")\n",
    "else:\n",
    "    print(\"Skipping Random Forest Analysis due to missing or empty df_ml.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91c577",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Visualizing the Lilliput Effect: Body Size vs. Climate\n",
    "This plot illustrates the relationship between mean body size and global temperature (inferred from the d18O proxy) over the Cenozoic Era. This helps us visually assess how body size responds to climatic shifts and whether the 'Lilliput Effect' is evident.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_ml' in locals() and not df_ml.empty:\n",
    "    # --- 1. Prepare Data for Plotting ---\n",
    "    # Group by Time Bin to get the global average size per 0.1 Ma\n",
    "    # This removes the \"noise\" of individual fossils and shows the global trend\n",
    "    plot_data = df_ml.groupby('time_bin', observed=False)[['average', 'd18o']].mean().reset_index()\n",
    "\n",
    "    # Convert time_bin back to numeric Age (midpoint)\n",
    "    plot_data['age_ma'] = plot_data['time_bin']\n",
    "\n",
    "    # Drop empty bins\n",
    "    plot_data = plot_data.dropna()\n",
    "\n",
    "    # Ensure plot_data is not empty after dropping NaNs\n",
    "    if not plot_data.empty:\n",
    "        # --- 2. Create the Dual-Axis Plot ---\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # A. Plot Body Size (The Target)\n",
    "        color_size = 'tab:blue'\n",
    "        ax1.set_xlabel('Time (Million Years Ago)')\n",
    "        ax1.set_ylabel('Mean Body Size (mm)', color=color_size, fontsize=12, fontweight='bold')\n",
    "        ax1.plot(plot_data['age_ma'], plot_data['average'], color=color_size, linewidth=2, label='Body Size')\n",
    "        ax1.tick_params(axis='y', labelcolor=color_size)\n",
    "        ax1.invert_xaxis() # Geologists always plot time from Right (Old) to Left (Young)\n",
    "\n",
    "        # B. Plot Temperature (The Predictor)\n",
    "        # We use a second Y-axis (right side)\n",
    "        ax2 = ax1.twinx()\n",
    "        color_temp = 'tab:red'\n",
    "        ax2.set_ylabel('Global Temperature Proxy (d18O)', color=color_temp, fontsize=12, fontweight='bold')\n",
    "        # Note: Higher d18O means COLDER. We invert this axis so UP = WARMER\n",
    "        ax2.plot(plot_data['age_ma'], plot_data['d18o'], color=color_temp, linestyle='--', alpha=0.6, label='Temperature (d18O)')\n",
    "        ax2.tick_params(axis='y', labelcolor=color_temp)\n",
    "        ax2.invert_yaxis()\n",
    "\n",
    "        # --- 3. Formatting ---\n",
    "        plt.title('The Lilliput Effect: Global Body Size vs. Climate (Cenozoic Era)', fontsize=14)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plotting skipped: plot_data is empty after cleaning.\")\n",
    "else:\n",
    "    print(\"Skipping Visualization of Lilliput Effect due to missing or empty df_ml.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097b35e",
   "metadata": {},
   "source": [
    "## ðŸ§  Neural ODEs: Learning Continuous Dynamics\n",
    "This section demonstrates the use of Neural Ordinary Differential Equations (Neural ODEs) to learn continuous dynamic systems from irregularly sampled time-series data. This technique is particularly valuable for paleontological records, which are often sparse and gappy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ed1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: INSTALLATION ---\n",
    "# We need the torchdiffeq library for the ODE solvers\n",
    "!pip install torchdiffeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import the ODE solver\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# --- STEP 2: GENERATE SYNTHETIC PALEO DATA (SINE WAVE WITH GAPS) ---\n",
    "# We simulate a \"perfect\" fossil record (sine wave) and then \"erode\" it\n",
    "# to create irregular sampling, mimicking real geological data.\n",
    "\n",
    "data_size = 1000\n",
    "batch_time = 20\n",
    "batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# True Dynamics (The \"Laws of Physics\" we want to learn)\n",
    "true_y0 = torch.tensor([[2., 0.]]).to(device) # Initial condition (Diversity, Rate)\n",
    "t = torch.linspace(0., 25., data_size).to(device) # Time (0 to 25 Ma)\n",
    "true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device) # Spiral dynamics Matrix\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y**3, true_A) # Cubic dynamics (non-linear)\n",
    "\n",
    "# Generate the \"True\" history (The ground truth)\n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method='dopri5')\n",
    "\n",
    "def get_batch():\n",
    "    # This function creates \"Irregular Gaps\"\n",
    "    # We grab random slices of time, simulating imperfect preservation\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(data_size - batch_time, dtype=np.int64), batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n",
    "\n",
    "# --- STEP 3: DEFINE THE NEURAL ODE (The \"ODEFunc\") ---\n",
    "# Instead of predicting Y directly, we predict the DERIVATIVE (dy/dt).\n",
    "# The Solver then integrates this to find Y.\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        # A simple MLP (Multi-Layer Perceptron) that approximates the derivative\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2), # Output is 2D (Diversity, Rate)\n",
    "        )\n",
    "\n",
    "        # Initialize weights for better convergence\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "# --- STEP 4: TRAINING LOOP ---\n",
    "func = ODEFunc().to(device)\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training Neural ODE... (Learning the underlying dynamics)\")\n",
    "start_time = time.time()\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for itr in range(1, 1001): # 1000 Iterations\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "\n",
    "    # FORWARD PASS: Integrate the Neural ODE from t0 to t_end\n",
    "    # This uses the 'dopri5' solver (Runge-Kutta)\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(device)\n",
    "\n",
    "    # Calculate Loss (Difference between predicted trajectory and true data)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if itr % 100 == 0:\n",
    "        print(f\"Iter {itr:04d} | Total Loss {loss.item():.6f}\")\n",
    "\n",
    "print(f\"Training Complete in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# --- STEP 5: VISUALIZATION (Verify Solver) ---\n",
    "# We verify if the model learned the \"Sine Wave\" shape despite the gaps.\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_y = odeint(func, true_y0, t)\n",
    "\n",
    "# Visualize the synthetic data and the learned trajectory\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 0], label='True Y (Dimension 0)')\n",
    "plt.plot(t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 0], '--', label='Predicted Y (Dimension 0)')\n",
    "plt.scatter(t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 0], s=5, alpha=0.5, label='Sampled Data')\n",
    "plt.title('Neural ODE: Learning Synthetic Dynamics')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cddb65",
   "metadata": {},
   "source": [
    "## ðŸ”® Time-Series Forecasting: Chronos-T5 for Biodiversity\n",
    "This section explores the application of Chronos-T5, a transformer-based foundation model for time-series forecasting, to predict future biodiversity trends based on the Cenozoic fossil record.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install chronos-forecasting --force-reinstall\n",
    "\n",
    "print(\"Chronos-forecasting re-installation initiated. This may take a moment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# --- 1. RELOAD DATA (In case variables were lost during restart) ---\n",
    "print(\"1. Preparing Fossil Time Series...\")\n",
    "try:\n",
    "    # Try loading processed data first\n",
    "    df_ml = pd.read_csv('Lilliput_Project_Data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'Lilliput_Project_Data.csv' not found. Attempting to re-process data.\")\n",
    "    # Fallback: Re-create from raw files\n",
    "    # This assumes raw files (pbdb_data.csv, pbdb_data (1).csv, TableS33.tab) are available\n",
    "    try:\n",
    "        df_context = pd.read_csv('pbdb_data.csv', skiprows=19)\n",
    "        df_meas = pd.read_csv('pbdb_data (1).csv', skiprows=17)\n",
    "        df_final = pd.merge(df_context, df_meas, on='specimen_no', how='inner')\n",
    "\n",
    "        df_climate = pd.read_csv(\n",
    "            'TableS33.tab',\n",
    "            sep='\\t',\n",
    "            skiprows=91\n",
    "        )\n",
    "        df_climate = df_climate.rename(columns={\n",
    "            'Tuned time [Ma]': 'age_ma',\n",
    "            'Foram benth Î´18O [â€° PDB] (VPDB CorrAdjusted)': 'd18o',\n",
    "            'Foram benth Î´13C [â€° PDB] (VPDB CorrAdjusted)': 'd13c'\n",
    "        })\n",
    "        df_climate = df_climate[(df_climate['age_ma'] >= 0) & (df_climate['age_ma'] <= 66)]\n",
    "\n",
    "        # Create midpoint age\n",
    "        if 'min_ma' in df_final.columns:\n",
    "            df_final['age_mid'] = (df_final['max_ma'] + df_final['min_ma']) / 2\n",
    "        else:\n",
    "            df_final['age_mid'] = df_final['max_ma']\n",
    "\n",
    "        # Define Time Bins (0.1 Ma slices)\n",
    "        bins = np.arange(0, 66.1, 0.1)\n",
    "        labels = bins[:-1] + 0.05\n",
    "        df_final['time_bin'] = pd.cut(df_final['age_mid'], bins=bins, labels=labels, right=False)\n",
    "        df_climate['time_bin'] = pd.cut(df_climate['age_ma'], bins=bins, labels=labels, right=False)\n",
    "        df_climate_binned = df_climate.groupby('time_bin', observed=False)['d18o'].mean().reset_index()\n",
    "        df_final = pd.merge(df_final, df_climate_binned, on='time_bin', how='left')\n",
    "\n",
    "        # Redo Feature Engineering steps to get df_ml\n",
    "        df_ml = df_final.copy()\n",
    "        df_ml = df_ml.dropna(subset=['average', 'd18o'])\n",
    "        df_ml = df_ml[df_ml['average'] > 0]\n",
    "        if 'paleolat' in df_ml.columns:\n",
    "            df_ml['latitude_abs'] = df_ml['paleolat'].abs()\n",
    "        elif 'lat' in df_ml.columns:\n",
    "            df_ml['latitude_abs'] = df_ml['lat'].abs()\n",
    "        else:\n",
    "            df_ml['latitude_abs'] = 0\n",
    "        df_ml['lith_clean'] = df_ml['lithology1'].astype(str).str.lower().str.replace('\"', '').str.strip()\n",
    "        top_10_rocks = df_ml['lith_clean'].value_counts().nlargest(10).index\n",
    "        df_ml['lith_simple'] = df_ml['lith_clean'].apply(lambda x: x if x in top_10_rocks else 'other')\n",
    "        lith_dummies = pd.get_dummies(df_ml['lith_simple'], prefix='lith')\n",
    "        df_ml = pd.concat([df_ml, lith_dummies], axis=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data reprocessing for Chronos: {e}\")\n",
    "        df_ml = pd.DataFrame() # Ensure df_ml is empty if reprocessing fails\n",
    "\n",
    "if 'df_ml' in locals() and not df_ml.empty:\n",
    "    # Create Time Series: Count unique genera per time step\n",
    "    # Sort descending (Oldest -> Youngest) because Chronos expects time to move forward\n",
    "    ts_data = df_ml.groupby('age_mid')['genus'].nunique().sort_index(ascending=False)\n",
    "\n",
    "    # --- 2. LOAD THE AI MODEL ---\n",
    "    print(\"2. Loading Chronos-T5 (Foundation Model)...\")\n",
    "    pipeline = ChronosPipeline.from_pretrained(\n",
    "        \"amazon/chronos-t5-small\",\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    # --- 3. RUN FORECAST ---\n",
    "    print(\"3. Forecasting the Future...\")\n",
    "    # Convert data to tensor\n",
    "    context = torch.tensor(ts_data.values)\n",
    "    prediction_length = 20 # Forecast 20 steps into the \"future\"\n",
    "\n",
    "    forecast = pipeline.predict(\n",
    "        context,\n",
    "        prediction_length,\n",
    "        num_samples=20,\n",
    "    )\n",
    "\n",
    "    # --- 4. VISUALIZE RESULTS ---\n",
    "    low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
    "    history_len = len(context)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plot History (Real Fossil Record)\n",
    "    plt.plot(np.arange(history_len), context, color=\"black\", label=\"History (Fossil Record)\")\n",
    "    # Plot Forecast (AI Prediction)\n",
    "    plt.plot(np.arange(history_len, history_len + prediction_length), median, color=\"blue\", label=\"AI Forecast\")\n",
    "    # Plot Uncertainty (Confidence Interval)\n",
    "    plt.fill_between(\n",
    "        np.arange(history_len, history_len + prediction_length),\n",
    "        low, high, color=\"blue\", alpha=0.3, label=\"10-90% Quantile\"\n",
    "    )\n",
    "\n",
    "    plt.title(\"Biodiversity Forecasting with Chronos-T5\", fontsize=14)\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Number of Unique Genera\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping Chronos-T5 initial forecasting due to missing or empty df_ml.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de98f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets\n",
    "\n",
    "print(\"Datasets library installation initiated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "if 'ts_data' in locals() and not ts_data.empty:\n",
    "    # 1. Convert ts_data (pandas Series) into a NumPy array.\n",
    "    full_series_np = ts_data.values\n",
    "\n",
    "    # 2. Define the length of the validation set (val_len)\n",
    "    val_len = 9 # The instructions suggest 9 points for ~20% of total data.\n",
    "\n",
    "    # 3. Split the full time series NumPy array into train_series and val_series.\n",
    "    train_series_np = full_series_np[:-val_len]\n",
    "    val_series_np = full_series_np[-val_len:]\n",
    "\n",
    "    print(f\"Total data points: {len(full_series_np)}\")\n",
    "    print(f\"Training series length: {len(train_series_np)}\")\n",
    "    print(f\"Validation series length: {len(val_series_np)}\")\n",
    "\n",
    "    # 4. Define context_window and prediction_window\n",
    "    context_window = 20 # Example: Observe 20 past data points\n",
    "    prediction_window = 5 # Example: Predict 5 future data points\n",
    "\n",
    "    # 5. Create a list of training samples.\n",
    "    train_samples = []\n",
    "    # Ensure there's enough data for at least one full sample\n",
    "    if len(train_series_np) >= context_window + prediction_window:\n",
    "        for i in range(len(train_series_np) - context_window - prediction_window + 1):\n",
    "            context = torch.tensor(train_series_np[i : i + context_window], dtype=torch.float32)\n",
    "            target = torch.tensor(train_series_np[i + context_window : i + context_window + prediction_window], dtype=torch.float32)\n",
    "            train_samples.append({'context': context, 'target': target})\n",
    "\n",
    "    print(f\"Generated {len(train_samples)} training samples.\")\n",
    "\n",
    "    # 6. Create a single validation sample.\n",
    "    # The context for this sample should be the entire train_series_np\n",
    "    # The target should be the entire val_series_np\n",
    "    val_context_tensor = torch.tensor(train_series_np, dtype=torch.float32)\n",
    "    val_target_tensor = torch.tensor(val_series_np, dtype=torch.float32)\n",
    "    val_sample = {'context': val_context_tensor, 'target': val_target_tensor}\n",
    "\n",
    "    # 7. Convert the list of training samples into a datasets.Dataset object.\n",
    "    train_dataset = Dataset.from_list(train_samples)\n",
    "\n",
    "    print(\"Data preparation for fine-tuning complete.\")\n",
    "    print(f\"Training Dataset: {train_dataset}\")\n",
    "    print(f\"Validation Sample Context Shape: {val_sample['context'].shape}\")\n",
    "    print(f\"Validation Sample Target Shape: {val_sample['target'].shape}\")\n",
    "else:\n",
    "    print(\"Skipping Chronos data preparation due to missing or empty ts_data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# 1. Instantiate the ChronosPipeline model\n",
    "print(\"Loading Chronos-T5 model for fine-tuning...\")\n",
    "pipeline_ft = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 2. Set the model to training mode\n",
    "pipeline_ft.model.train()\n",
    "print(\"Model set to training mode.\")\n",
    "\n",
    "# 3. Define the prediction_length for training\n",
    "prediction_length_ft = prediction_window # prediction_window should be defined in the previous cell\n",
    "print(f\"Prediction length for fine-tuning set to: {prediction_length_ft}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if 'pipeline_ft' in locals() and 'train_series_np' in locals() and not train_series_np.empty:\n",
    "    # Access the inner Hugging Face T5 model for training\n",
    "    inner_model = pipeline_ft.model.model\n",
    "    inner_model.train()\n",
    "\n",
    "    optimizer = AdamW(inner_model.parameters(), lr=1e-5)\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Explicitly set the tokenizer's prediction_length to match the prediction_window\n",
    "    pipeline_ft.tokenizer.config.prediction_length = prediction_window\n",
    "\n",
    "    # Ensure train_data is correctly referenced (it should be train_series_np)\n",
    "    train_data_tensor = torch.tensor(train_series_np, dtype=torch.float32)\n",
    "\n",
    "    print(\"Starting fine-tuning...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0 # To calculate average loss per epoch\n",
    "\n",
    "        # Adjust max_start_idx to account for both context and prediction windows\n",
    "        max_start_idx = len(train_data_tensor) - context_window - prediction_window\n",
    "        if max_start_idx < 1:\n",
    "            print(f\"Skipping epoch {epoch+1} due to insufficient data for context_window {context_window} and prediction_window {prediction_window}\")\n",
    "            # Define avg_loss here to prevent NameError if epoch is skipped\n",
    "            avg_loss = 0.0 # Or some appropriate default\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "            continue\n",
    "\n",
    "        for _ in tqdm(range(max_start_idx), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. Sample a random window\n",
    "            # The start_idx now refers to the beginning of the context window\n",
    "            start_idx = np.random.randint(0, max_start_idx)\n",
    "            window_context = train_data_tensor[start_idx : start_idx + context_window]\n",
    "            window_target = train_data_tensor[start_idx + context_window : start_idx + context_window + prediction_window]\n",
    "\n",
    "            # Add a batch dimension (unsqueeze(0)) for both window and target\n",
    "            window_context = window_context.unsqueeze(0)\n",
    "            window_target = window_target.unsqueeze(0)\n",
    "\n",
    "            # 2. Tokenize using the pipeline's tokenizer\n",
    "            input_ids, attention_mask, scale = pipeline_ft.tokenizer.context_input_transform(window_context)\n",
    "\n",
    "            # label_input_transform uses the SAME scale to tokenize the target\n",
    "            # The length of window_target MUST be equal to prediction_length, which is prediction_window\n",
    "            # Adjusted to unpack only 2 values as per ValueError\n",
    "            labels, _ = pipeline_ft.tokenizer.label_input_transform(window_target, scale)\n",
    "\n",
    "            # Move to GPU/CPU\n",
    "            device = inner_model.device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 3. Forward Pass & Update using the INNER MODEL\n",
    "            outputs = inner_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / max_start_idx\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Fine-tuning complete!\")\n",
    "else:\n",
    "    print(\"Skipping Chronos fine-tuning due to missing pipeline_ft or empty train_series_np.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if 'ts_data' in locals() and not ts_data.empty and 'pipeline_ft' in locals():\n",
    "    # --- 1. SETUP VALIDATION DATA ---\n",
    "    # We split the data again: 90% History (Context) vs 10% Future (Truth)\n",
    "    split_idx = int(len(ts_data) * 0.9)\n",
    "    train_data = torch.tensor(ts_data.values[:split_idx], dtype=torch.float32) # The Past\n",
    "    valid_data = torch.tensor(ts_data.values[split_idx:], dtype=torch.float32) # The \"Hidden\" Future\n",
    "\n",
    "    print(f\"Forecasting the last {len(valid_data)} time steps (The 'Hidden' Future)...\")\n",
    "\n",
    "    # --- 2. SWITCH TO INFERENCE MODE ---\n",
    "    pipeline_ft.model.eval() # Stop updating weights\n",
    "\n",
    "    # --- 3. GENERATE PREDICTION ---\n",
    "    # We feed it the History (train_data) and ask for the Future\n",
    "    forecast = pipeline_ft.predict(\n",
    "        train_data,\n",
    "        len(valid_data),\n",
    "        num_samples=20\n",
    "    )\n",
    "\n",
    "    # --- 4. VISUALIZE THE RESULTS ---\n",
    "    low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # A. Plot the History (Black)\n",
    "    plt.plot(np.arange(split_idx), train_data, color=\"black\", label=\"Training History\")\n",
    "\n",
    "    # B. Plot the TRUE Future (Green Dashed) - This is what actually happened\n",
    "    plt.plot(np.arange(split_idx, len(ts_data)), valid_data, color=\"green\", linestyle=\"--\", linewidth=2, label=\"True Future (Hidden)\")\n",
    "\n",
    "    # C. Plot the AI Forecast (Red) - This is what the Fine-Tuned AI predicted\n",
    "    plt.plot(np.arange(split_idx, len(ts_data)), median, color=\"red\", linewidth=2, label=\"Fine-Tuned Forecast\")\n",
    "    plt.fill_between(\n",
    "        np.arange(split_idx, len(ts_data)),\n",
    "        low, high, color=\"red\", alpha=0.2, label=\"Uncertainty\"\n",
    "    )\n",
    "\n",
    "    plt.title(\"The Final Test: Can the Fine-Tuned AI Predict the Cenozoic?\", fontsize=14)\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Biodiversity\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 5. Calculate Chronos Accuracy ---\n",
    "    forecast_median = np.quantile(forecast[0].numpy(), 0.5, axis=0)\n",
    "    chronos_mse = mean_squared_error(valid_data.numpy(), forecast_median)\n",
    "\n",
    "    print(f\"--- FINAL SCOREBOARD ---\")\n",
    "    print(f\"Chronos (Fine-Tuned Model) MSE: {chronos_mse:.4f}\")\n",
    "\n",
    "    print(\"\\n--- HOW TO JUDGE THE WINNER ---\")\n",
    "    print(f\"Your Fine-Tuned Chronos Error is: {chronos_mse:.4f}\")\n",
    "    print(\"Compare this to your Neural ODE visual fit.\")\n",
    "    print(\"- If Chronos MSE is < 0.05 (normalized), the 'Big Data' approach won.\")\n",
    "    print(\"- If Chronos MSE is > 0.10, the 'Physics' approach (Neural ODE) likely won.\")\n",
    "\n",
    "    # --- 6. Save Final Prediction for Report ---\n",
    "    results_df = pd.DataFrame({\n",
    "        'Time_Step': np.arange(len(valid_data)),\n",
    "        'True_Diversity': valid_data.numpy(),\n",
    "        'AI_Predicted_Diversity': forecast_median\n",
    "    })\n",
    "    results_df.to_csv('Final_Model_Comparison_FineTuned.csv', index=False)\n",
    "    print(\"\\n-> Results saved to 'Final_Model_Comparison_FineTuned.csv'\")\n",
    "else:\n",
    "    print(\"Skipping Chronos forecasting evaluation due to missing ts_data or pipeline_ft.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7b0ac",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Conclusion\n",
    "This notebook explored the Lilliput Effect using a multi-faceted approach, combining traditional machine learning with advanced deep learning techniques.\n",
    "\n",
    "### Key Findings:\n",
    "*   **Random Forest Analysis:** Identified absolute latitude as the primary driver of body size, followed by climate (d18O), suggesting strong geographic and ecological signals. Lithology also played a role, pointing to potential taphonomic influences.\n",
    "*   **Lilliput Effect Visualization:** Visually demonstrated a general inverse relationship between global temperature and mean body size, supporting the Lilliput Effect, though with caveats for linearity and synchronicity.\n",
    "*   **Neural ODEs:** Introduced as a powerful method for learning continuous dynamics from sparse, irregularly sampled data, highly relevant for paleontological research.\n",
    "*   **Chronos-T5 Forecasting:** While providing an initial forecast of biodiversity, the fine-tuned Chronos-T5 model achieved an MSE of 13.0511. This high error suggests that for this specific paleontological time series, a \"physics-based\" approach (like Neural ODEs) might be more suitable than this \"Big Data\" model in its current fine-tuned state.\n",
    "\n",
    "### Overall Insights:\n",
    "The study highlights the complex interplay of environmental and geological factors in shaping ancient life. While machine learning can identify correlations, models capable of learning underlying dynamic processes (such as Neural ODEs) may offer deeper insights into the mechanisms driving macroevolutionary trends in sparse paleontological datasets. Further research should focus on integrating these diverse modeling approaches to gain a more comprehensive understanding of deep-time biological phenomena.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
